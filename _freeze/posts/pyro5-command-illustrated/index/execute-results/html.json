{
  "hash": "c6c207f90661c388ece345c3971a1bce",
  "result": {
    "markdown": "---\ntitle: \"Annotated pyro5 command\"\nauthor: \"Linh Pham\"\ndescription: \"How to run PydPiper using the Pyro5 backend, as demonstrated by the Tamarack pipeline\"\ndate: \"2025/07/29\"\ndate-modified: \"2025-07-29\"\ncategories: [pydpiper, Oxford, longitudinal]\ndraft: false\nengine: knitr\nnumber-sections: false\n---\n\n\nPydPiper and its longitudinal registration pipeline sibling, Tamarack, can use either [Makeflow](https://users.ox.ac.uk/~ndcn0890/posts/MBM-command-illustrated/) or Pyro5 as its their back end workflow managers. The results will be the same regardless of which back end is used, though I do find it easier to monitor pipeline progress and debug errors using Pyro5.\n\nMakeflow may also crash with large number of images in the pipeline due to a [deep recursion within its code](https://github.com/cooperative-computing-lab/cctools/issues/2341) (a function calls itself again many times, which can happen when there are many files and task dependencies in a pipeline).\n\nThis can be fixed by changing the soft stack limit prior to running the pipeline with makeflow.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nulimit -s unlimited\n```\n:::\n\n\nThe soft stack limit can also be increased before running Pyro5, just in case, but it likely won't need that command as much as the Makeflow backend.\n\n# Annotated Tamarack command with Pyro5 back end\n\nWith all that said, here's how to use the Pyro5 back end on the Oxford BMRC. I'm currently working with longitudinal data, so the command is `registration_tamarack.py`, but it can also be `MBM.py` if you're working with cross-sectional data.\n\nI'm putting comments next to the main differences between running with makeflow vs. Pyro5 back ends and cross-sectional vs. longitudinal registration settings.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nmodule load parallel \\ # <1>\nmodule use --append /well/lerch/shared/tools/modules/ \\\nmodule load minctoolkit \\\nconda activate pydpiper \\\n\nexport APPTAINER_BIND=/well/,/gpfs3 \\\n\nregistration_tamarack.py \\ # <2> \n--pipeline-name sample-registration \\ \n--maget-registration-method minctracc \\ \n--subject-matter mousebrain \\ \n--csv-file /path_to_your_registration_csv/sample_registration.csv \\ # <3>\n--pride-of-models /path_to_your_pride_of_model/sample_pride_of_models.csv \\ # <4> \n--common-time-point 80 \\ # <5> \n--run-maget --maget-atlas-library /well/lerch/shared/tools/atlases/Dorr_2008_Steadman_2013_Ullmann_2013/in-vivo-MEMRI_90micron/ \\  \n--maget-nlin-protocol /well/lerch/shared/tools/protocols/nonlinear/default_nlin_MAGeT_minctracc_prot.csv \\ \n--maget-masking-nlin-protocol /well/lerch/shared/tools/protocols/nonlinear/default_nlin_MAGeT_minctracc_prot.csv \\\n--lsq12-protocol /well/lerch/shared/tools/protocols/linear/Pydpiper_testing_default_lsq12.csv --lsq6-simple \\ # <6>\n--num-executors 26 \\ # <7> \n--mem 20 \\ # <8>\n--queue-type slurm \\ # <9>\n--queue-name 'short,win' \\ # <10>\n--use-singularity  \\ # <11>\n--container-path /well/lerch/shared/tools/mice.sif_latest.sif \\ # <12>\n```\n:::\n\n\n1.  Lines to call necessary modules, tools, and container needed to run the pipeline. Comparing to the Makeflow back end, we don't need to do `alias SE='apptainer exec /well/lerch/shared/tools/mice.sif_latest.sif'` prior to calling the registration command.\n2.  The command to register images from a longitudinal data set. Notice we don't need the flag `--backend makeflow` because the default back end for `registration_tamarack.py` and `MBM.py` is actually Pyro5.\n3.  A csv containing names of files to be registered in this pipeline. Filename denotes the location of the image to be registered using absolute paths. Group denotes the time point for which the image will be aligned toward during LSQ6 and non-linear registration stages. Snippet of an example csv is shown in the next section.\n4.  A csv file containing the locations of initial models for the pipeline. Because we are registering the images longitudinally, there are multiple initial models. Each initial model serves as the LSQ6 alignment starting goal for images that fall under that time point. An example csv is shown in the next section.\n5.  The common time point for image registration. Usually is the oldest age in the data set.\n6.  LSQ6 initial alignment choice. Chosen because all images fed into the longitudinal pipeline are already in LSQ6 space.\n7.  Number of executors. Usually set to be the same as the number of files being registered.\n8.  Memory in gigabyte assigned per task in the pipeline. Can be increased if needed. A trial and error process in my experience. I don't follow any hard and fast rule -- if pipeline is running too slow, then I increase this number.\n9.  The type of job scheduler for the cluster. The options are `pbs`, `sge`, `slurm`. BMRC uses `slurm`.\n10. The cluster partitions where pipeline tasks are run. On BMRC, we use the `short` and `win` partitions.\n11. Flag to indicate that our tools and pipelines live inside a singularity container, and that the pipeline should be ran inside the singularity container. This allows us to not set alias SE and execute our container (see comment 1).\n12. Path to our singularity container.\n\n# Sample CSVs needed for Tamarack pipeline\n\nSnippet of a sample registration CSV.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\nsubject_id,treatment,filename,data_origin,group\n03_01,CTL,/well/lerch/users/xrs336/autessa-lsq6/sub-MCH_NEO_HIT_03_01_ses-1_FLASH_denoise_N_I_lsq6.mnc,autessa,5\n03_01,CTL,/well/lerch/users/xrs336/autessa-lsq6/sub-MCH_NEO_HIT_03_01_ses-2_FLASH_denoise_N_I_lsq6.mnc,autessa,5\n03_01,CTL,/well/lerch/users/xrs336/autessa-lsq6/sub-MCH_NEO_HIT_03_01_ses-3_FLASH_denoise_N_I_lsq6.mnc,autessa,7\n03_01,CTL,/well/lerch/users/xrs336/autessa-lsq6/sub-MCH_NEO_HIT_03_01_ses-4_FLASH_denoise_N_I_lsq6.mnc,autessa,7\n03_02,CTL,/well/lerch/users/xrs336/autessa-lsq6/sub-MCH_NEO_HIT_03_02_ses-1_FLASH_denoise_N_I_lsq6.mnc,autessa,5\n03_02,CTL,/well/lerch/users/xrs336/autessa-lsq6/sub-MCH_NEO_HIT_03_02_ses-2_FLASH_denoise_N_I_lsq6.mnc,autessa,5\n03_02,CTL,/well/lerch/users/xrs336/autessa-lsq6/sub-MCH_NEO_HIT_03_02_ses-3_FLASH_denoise_N_I_lsq6.mnc,autessa,7\n03_02,CTL,/well/lerch/users/xrs336/autessa-lsq6/sub-MCH_NEO_HIT_03_02_ses-4_FLASH_denoise_N_I_lsq6.mnc,autessa,7\n03_03,CTL,/well/lerch/users/xrs336/autessa-lsq6/sub-MCH_NEO_HIT_03_03_ses-1_FLASH_denoise_N_I_lsq6.mnc,autessa,5\n```\n:::\n\n\nPride of models CSV.\n\n\n::: {.cell}\n\n```{.bash .cell-code}\ntime_point,model_file\n3,/well/lerch/users/xrs336/test_initial_models/all-cohorts-test-run-070225/p03/p03_mouse_brain.mnc\n5,/well/lerch/users/xrs336/test_initial_models/all-cohorts-test-run-070225/p05/p05_mouse_brain.mnc\n7,/well/lerch/users/xrs336/test_initial_models/all-cohorts-test-run-070225/p07/p07_mouse_brain.mnc\n10,/well/lerch/users/xrs336/test_initial_models/all-cohorts-test-run-070225/p10/p10_mouse_brain.mnc\n17,/well/lerch/users/xrs336/test_initial_models/all-cohorts-test-run-070225/p17/p17_mouse_brain.mnc\n23,/well/lerch/users/xrs336/test_initial_models/all-cohorts-test-run-070225/p23/p23_mouse_brain.mnc\n36,/well/lerch/users/xrs336/test_initial_models/all-cohorts-test-run-070225/p36/p36_mouse_brain.mnc\n65,/well/lerch/users/xrs336/test_initial_models/all-cohorts-test-run-070225/p65/p65_mouse_brain.mnc\n80,/well/lerch/users/xrs336/test_initial_models/all-cohorts-test-run-070225/p80/p80_mouse_brain.mnc\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}